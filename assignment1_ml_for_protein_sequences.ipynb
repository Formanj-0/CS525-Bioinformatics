{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b2bd38d",
   "metadata": {},
   "source": [
    "# CS 525 Assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f15199-c68d-4f70-9fc7-346355ccb4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5c5af0",
   "metadata": {},
   "source": [
    "### Machine learning for protein sequence data\n",
    "\n",
    "In this assignment you will explore the approach of modeling sequence data using feature representations based on k-mer composition.  Our data will come from the SCOP database, which is a curated repository of proteins classified according to the overall characteristics of their 3-d structure.  We will explore how well this information can be predicted from the amino acid sequence.\n",
    "\n",
    "The primary tool we will use is the k-mer composition vector, which represents a protein in the space of all k-mers of length $k$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a8bac5",
   "metadata": {},
   "source": [
    "### Preliminaries:  data\n",
    "\n",
    "In this assignment you will create classifiers that predict the SCOP family a sequence belongs to.  The [SCOP database](https://scop.mrc-lmb.cam.ac.uk/) classifies sequences into families based on the characteristics of their 3-d structure.  The identifier of a SCOP family has four component, and is of the form **w.x.y.z**, representing the class.fold.superfamily.family levels of organization in SCOP (e.g. b.1.1.0).\n",
    "These four levels of classification used by SCOP are:  \n",
    "* family:  a group of closely related proteins\n",
    "* superfamily:  groups together several families of proteins that likely share a common evolutionary origin\n",
    "* fold:  groups together superfamilies based on structural properties\n",
    "* class:  combine folds with common secondary secondary structure.  These include all-alpha and all-beta proteins, containing predominantly alpha-helices and beta-strands, respectively, and ‘mixed’ alpha and beta classes (a/b) and (a+b) with respectively alternating and segregated alpha-helices and beta-strands.\n",
    "\n",
    "In your experiments you will focus on good sized families: 'c.2.1.2' and 'c.2.1.0' which are proteins with a mix of alpha helix and beta sheet secondary structures, and hence the \"c\".  Both families belong to the c.2.1 superfamily of NAD(P) binding proteins.  You can obtain the sequences from the following fasta file:\n",
    "\n",
    "https://scop.berkeley.edu/downloads/scopeseq-2.08/astral-scopedom-seqres-gd-sel-gs-bib-95-2.08.fa\n",
    "\n",
    "The file contains the sequences in version 2.08 of SCOP, processed to contain a non-redundant set of sequences such that no two sequences share more than 95% sequence identity.  This file contains the entire set of sequences, and you will need to extract from it those sequences that belong to the two classes we are interested in.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4581a1-ae4a-4371-a957-0e3f52163b5d",
   "metadata": {},
   "source": [
    "### Sparse arrays\n",
    "\n",
    "As you have seen in class, k-mer composition can lead to sparse feature representations.  As a result, standard arrays are not ideal for storing such data.  The scipy package provides implementations of several classes of sparse arrays in the [sparse module](https://docs.scipy.org/doc/scipy/reference/sparse.html).\n",
    "To create our sparse array it will be convenient to first use the [dok_array](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.dok_array.html#scipy.sparse.dok_array) sparse array which provides convenient functionality for incremental creation of the array.  We will then convert it to the format that scikit-learn likes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b41bca-cda4-4c55-954b-0302d09d7c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "# create a 2 x 5 empty sparse array\n",
    "x = sparse.dok_array((2,5))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56268e49-181d-4acc-bf25-53a0e69c0e39",
   "metadata": {},
   "source": [
    "Next we can populate the array with elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d40bf0-689f-42c4-8df0-f66567b37e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0,1]=1\n",
    "x[1,2]=1\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6671bf-5b18-48d8-989b-08204ed75ebd",
   "metadata": {},
   "source": [
    "Let's verify that the array looks as we would expect by converting it to a regular array:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd07c5c-f863-40f2-8e2b-f9aad39e39cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c319883a-91d5-450f-ba5a-0fea4a25cf6e",
   "metadata": {},
   "source": [
    "scikit-learn cannot work with this array format, so we need to convert it to a Compressed Sparse Row or CSR array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1d1735-426b-44fa-8d50-8da431838bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = x.tocsr()\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8e45ef-aca5-4420-8199-f498c1e67948",
   "metadata": {},
   "source": [
    "This format of sparse array supports efficient computation of dot products, which is what is needed for training an SVM.  As a a sidenote, the scipy array gets converted to the C++ data structure used by the SVM solver used in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4956a8f2-5cc3-4ea9-9937-e1ec8b86cb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "s = svm.SVC()\n",
    "y = np.array([0, 1])\n",
    "s.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32e87c7",
   "metadata": {},
   "source": [
    "### Part 1:  initial explorations with k-mer composition\n",
    "\n",
    "Throughout the assignment we will work with k-mer composition as features for representing protein sequences.  We will define k-mer composition as a vector that specifies the number of times each k-mer of length $k$ occurs in the sequence.\n",
    "\n",
    "\n",
    "In this first part of the assignment we will explore the effect of the choice of kernel (linear vs Gaussian).  We will do this for a fixed value of the k-mer size, $k=3$.  After we have found good choices for the kernel and pre-processing, we will explore performance as a function of $k$ in the next part of the assignment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3357a91",
   "metadata": {},
   "source": [
    "#### Baseline and data normalization.\n",
    "\n",
    "Like other weight-vector based classifiers, SVMs benefit from appropriate normalization of the data.  Since k-mer composition data is sparse, standardization is not a good option.  Instead, we will convert the k-mer composition vectors into unit vectors.\n",
    "This is done by dividing each row in the feature matrix by its norm.\n",
    "Recall that Euclidean the norm of a $d$ dimensional vector $\\mathbf{x}$ is defined by:\n",
    "$\n",
    "||\\mathbf{x}||^2 = \\sum_{i=1}^d x_i^2.\n",
    "$\n",
    "You can verify that after this operation each row in the feature matrix has unit norm.\n",
    "\n",
    "As a baseline, compute the accuracy of a linear SVM on the normalized feature vectors.\n",
    "Classifier hyper-parameters (the value of $C$) should be chosen using grid search.\n",
    "Since the dataset is not very large, nested cross-validation as performed in our first exercise is a good approach approach.  Since the dataset is not as small, you can opt for five-fold cross-validation rather than ten-fold as we have done previously.\n",
    "\n",
    "Since the data is somewhat unbalanced, use the [area under the precision recall curve](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html) to measure classifier accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6a7028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in data\n",
    "path = r'C:\\Users\\Jack\\Documents\\GitHub\\CS525-Bioinformatics\\astral-scopedom-seqres-gd-sel-gs-bib-95-2.08.fa'\n",
    "fasta_sequences = SeqIO.parse(open(path),'fasta')\n",
    "\n",
    "names = []\n",
    "sequences = []\n",
    "descriptions = []\n",
    "scop = []\n",
    "for fasta in fasta_sequences:\n",
    "    name, sequence, description = fasta.id, str(fasta.seq), str(fasta.description)\n",
    "    names.append(name)\n",
    "    sequences.append(sequence)\n",
    "    descriptions.append(description)\n",
    "    scop.append(description.split(' ')[1])\n",
    "\n",
    "i = 10\n",
    "print(names[i])\n",
    "print(sequences[i])\n",
    "print(descriptions[i])\n",
    "print(scop[i])\n",
    "num_seq = len(sequences)\n",
    "print(f'Number of sequences: {num_seq}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952211b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter data so that 'c.2.1.2' and 'c.2.1.0' are the only proteins\n",
    "cls_types = ['c.2.1.2', 'c.2.1.0']\n",
    "Y = []\n",
    "good_index = []\n",
    "\n",
    "for i, cls in enumerate(scop): # I know I shouldn't use cls\n",
    "    if cls in cls_types:\n",
    "        Y.append(cls_types.index(cls))\n",
    "        good_index.append(i)\n",
    "\n",
    "names = [names[i] for i in good_index]\n",
    "sequences = [sequences[i] for i in good_index]\n",
    "descriptions = [descriptions[i] for i in good_index]\n",
    "scop = [scop[i] for i in good_index]\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be96404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the set of kmers\n",
    "k = 3\n",
    "kmers = [S[i:i+k] for S in sequences for i in range(len(S) - k + 1)] # Note: for loops go from outside loop to inside\n",
    "print(f'All kmers: {len(kmers)}')\n",
    "kmers = set(kmers)\n",
    "num_kmers = len(kmers)\n",
    "print(f'unique set of kmers: {num_kmers}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225d204a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# count the number of occurances of each kmer (sparse matrix)\n",
    "X = sparse.dok_array((num_seq, num_kmers))\n",
    "\n",
    "kmer_index = {kmer: idx for idx, kmer in enumerate(kmers)}\n",
    "\n",
    "for i_s, S in enumerate(sequences):\n",
    "    kmer_counts = Counter([S[i:i+k] for i in range(len(S) - k + 1)]) # I wrote a crappy algorithm I then asked chat gpt to make it faster\n",
    "    for kmer, count in kmer_counts.items():                          # I was originally doing for loops iterating through each dimenstion and count \n",
    "        X[i_s, kmer_index[kmer]] = count                             # so I think that was O(num_seq*O*num_kmers*seq_length).\n",
    "                                                                     # I wonder what this is and what is the fastest.\n",
    "\n",
    "X = X[good_index, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f914a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'X Shape {X.shape}')\n",
    "print(f'Y Shape {Y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07691e9c",
   "metadata": {},
   "source": [
    "#### Comparison to Gaussian kernel and random forests\n",
    "\n",
    "Compare the linear SVM with a Gaussian kernel for the normalized k-mer composition representation with $k=3$.  As before, classifier hyper-parameters should be chosen using grid search.\n",
    "Is there a kernel that seems to work better?  Use it for the rest of your experiments.\n",
    "Finally, compare your SVM results with a random forests classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af07bfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import time\n",
    "\n",
    "\n",
    "num_of_trees = [1,2,5,10,50,100,300,500,1000]\n",
    "score = []\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "\n",
    "for n in num_of_trees:\n",
    "    tree = RandomForestClassifier(n_estimators=n)\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "    \n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    for train_index, test_index in cv.split(X, Y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = Y[train_index], Y[test_index]\n",
    "        \n",
    "        tree.fit(X_train, y_train)\n",
    "        \n",
    "        train_scores.append(tree.score(X_train, y_train))\n",
    "        test_scores.append(tree.score(X_test, y_test))\n",
    "\n",
    "    train_accuracy.append(np.mean(train_scores))\n",
    "    test_accuracy.append(np.mean(test_scores))\n",
    "\n",
    "    print(f\"Number of Trees: {n}, Train accuracy: {train_accuracy[-1]}, Test accuracy: {test_accuracy[-1]}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Time taken for {n} trees: {end_time - start_time} seconds\")\n",
    "\n",
    "plt.plot(num_of_trees, train_accuracy, label='Training Accuracy')\n",
    "plt.plot(num_of_trees, test_accuracy, label='Testing Accuracy')\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0e1ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "param_grid_gaussian = [\n",
    "  {'C': [0.1, 1, 10, 100, 1000], \n",
    "   'gamma': [0.0001, 0.001, 0.01, 0.1]},\n",
    " ]\n",
    "\n",
    "param_grid_linear = [\n",
    "  {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    " ]\n",
    "\n",
    "param_grid_tree = [\n",
    "    {'n_estimators': [1,2,5,10,50,100,300,500,1000]}\n",
    "]\n",
    "\n",
    "gaussian = GridSearchCV(svm.SVC(kernel='rbf'), param_grid_gaussian, cv=cv)\n",
    "linear = GridSearchCV(svm.SVC(kernel='linear'), param_grid_linear, cv=cv)\n",
    "tree = GridSearchCV(RandomForestClassifier(), param_grid=param_grid_tree, cv=cv)\n",
    "\n",
    "# linear.fit(X,Y)\n",
    "# gaussian.fit(X, Y)\n",
    "# tree.fit(X,Y)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.5, random_state=0\n",
    ")\n",
    "print('X training shape ', X_train.shape)\n",
    "print('X testing shape ', X_test.shape)\n",
    "print('Y training shape ', Y_train.shape)\n",
    "print('Y testing shape ', Y_test.shape)\n",
    "\n",
    "linear.fit(X_train, Y_train)\n",
    "gaussian.fit(X_train, Y_train)\n",
    "tree.fit(X_train, Y_train)\n",
    "\n",
    "print('guassian classifier: ', gaussian.best_params_)\n",
    "print('linear classifier: ', linear.best_params_)\n",
    "print('Tree classifier: ', tree.best_params_)\n",
    "\n",
    "display = PrecisionRecallDisplay.from_estimator(\n",
    "    gaussian, X_test, Y_test, name=\"GaussianSVM\", plot_chance_level=True, despine=True\n",
    ")\n",
    "_ = display.ax_.set_title(\"Gaussian Precision-Recall curve\")\n",
    "\n",
    "display = PrecisionRecallDisplay.from_estimator(\n",
    "    linear, X_test, Y_test, name=\"LinearSVM\", plot_chance_level=True, despine=True\n",
    ")\n",
    "_ = display.ax_.set_title(\"Linear Precision-Recall curve\")\n",
    "\n",
    "display = PrecisionRecallDisplay.from_estimator(\n",
    "    tree, X_test, Y_test, name=\"Tree\", plot_chance_level=True, despine=True\n",
    ")\n",
    "_ = display.ax_.set_title(\"Tree Precision-Recall curve\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6825f2",
   "metadata": {},
   "source": [
    "### Part 2:  performance as a function of k-mer size\n",
    "\n",
    "Compute the accuracy of the linear SVM as a function the k-mer size, $k$ starting with $k=1$.  Go as high as memory will reasonably allow.\n",
    "\n",
    "What value of the k-mer size gives you the best accuracy?\n",
    "Accuracy should increase, and depending on how high of a value of $k$ you are able to use, it will begin to decrease.\n",
    "Explain why you think this behavior is occuring.\n",
    "For each value of $k$, compute the dot-product matrix and visualize it.\n",
    "I recommend a grayscale visualization with \n",
    "```Python\n",
    "plt.imshow(K, cmap='Greys');\n",
    "```\n",
    "where $K$ is the matrix of dot products.\n",
    "Note that for a dataset with $N$ training examples, this will be an $N \\times N$ matrix.  The entry $i,j$ of $K$ is equal to the dot product $\\mathbf{x}_i^T \\mathbf{x}_j$, where $\\mathbf{x}_i$ is the feature vector of the $i$ th protein in the dataset.  Keep in mind that the computation can be performed in one line of NumPy code with no for loops!\n",
    "Before computing the matrix, **make sure the data is ordered according to the class labels**.  What patterns do you observe, and can that help you in understanding the performance of the resulting SVMs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ac3932",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_a_k(k):\n",
    "    print(f'=================== kmers of lenght: {k} ===================')\n",
    "    # generate the set of kmers\n",
    "    kmers = [S[i:i+k] for S in sequences for i in range(len(S) - k + 1)] # Note: for loops go from outside loop to inside\n",
    "    print(f'All kmers: {len(kmers)}')\n",
    "    kmers = set(kmers)\n",
    "    num_kmers = len(kmers)\n",
    "    print(f'unique set of kmers: {num_kmers}')\n",
    "\n",
    "    # count the number of occurances of each kmer (sparse matrix)\n",
    "    X = sparse.dok_array((num_seq, num_kmers))\n",
    "    kmer_index = {kmer: idx for idx, kmer in enumerate(kmers)}\n",
    "    for i_s, S in enumerate(sequences):\n",
    "        kmer_counts = Counter([S[i:i+k] for i in range(len(S) - k + 1)]) # I wrote a crappy algorithm I then asked chat gpt to make it faster\n",
    "        for kmer, count in kmer_counts.items():                          # I was originally doing for loops iterating through each dimenstion and count \n",
    "            X[i_s, kmer_index[kmer]] = count                             # so I think that was O(num_seq*O*num_kmers*seq_length).\n",
    "                                                                        # I wonder what this is and what is the fastest.\n",
    "    X = X[good_index, :]\n",
    "\n",
    "    print(f'X Shape {X.shape}')\n",
    "    print(f'Y Shape {Y.shape}')\n",
    "\n",
    "    # Find optimum parameters for each classifier\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "    param_grid_gaussian = [\n",
    "    {'C': [0.1, 1, 10, 100, 1000], \n",
    "    'gamma': [0.0001, 0.001, 0.01, 0.1]},\n",
    "    ]\n",
    "    param_grid_linear = [\n",
    "    {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "    ]\n",
    "    param_grid_tree = [\n",
    "        {'n_estimators': [1,2,5,10,50,100,300,500,1000]}\n",
    "    ]\n",
    "\n",
    "    gaussian = GridSearchCV(svm.SVC(kernel='rbf'), param_grid_gaussian, cv=cv)\n",
    "    linear = GridSearchCV(svm.SVC(kernel='linear'), param_grid_linear, cv=cv)\n",
    "    tree = GridSearchCV(RandomForestClassifier(), param_grid=param_grid_tree, cv=cv)\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.5, random_state=0\n",
    "    )\n",
    "\n",
    "    linear.fit(X,Y)\n",
    "    gaussian.fit(X, Y)\n",
    "    tree.fit(X,Y)\n",
    "\n",
    "    print('guassian classifier: ', gaussian.best_params_)\n",
    "    print('linear classifier: ', linear.best_params_)\n",
    "    print('Tree classifier: ', tree.best_params_)\n",
    "\n",
    "\n",
    "    # display the PrecisionRecallDisplay\n",
    "    display = PrecisionRecallDisplay.from_estimator(\n",
    "        gaussian, X_test, Y_test, name=\"GaussianSVM\", plot_chance_level=True, despine=True\n",
    "    )\n",
    "    _ = display.ax_.set_title(\"Gaussian Precision-Recall curve\")\n",
    "    plt.show()\n",
    "\n",
    "    display = PrecisionRecallDisplay.from_estimator(\n",
    "        linear, X_test, Y_test, name=\"LinearSVM\", plot_chance_level=True, despine=True\n",
    "    )\n",
    "    _ = display.ax_.set_title(\"Linear Precision-Recall curve\")\n",
    "    plt.show()\n",
    "\n",
    "    display = PrecisionRecallDisplay.from_estimator(\n",
    "        tree, X_test, Y_test, name=\"Tree\", plot_chance_level=True, despine=True\n",
    "    )\n",
    "    _ = display.ax_.set_title(\"Tree Precision-Recall curve\")\n",
    "    plt.show()\n",
    "    print('completed: kmers of lenght {k}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61244870",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = [1, 3, 5]\n",
    "for k in K:\n",
    "    try_a_k(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8133f823",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
