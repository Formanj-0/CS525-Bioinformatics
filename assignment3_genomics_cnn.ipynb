{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbde3611",
   "metadata": {},
   "source": [
    "## Assignment 3:  Convolutional networks for TF ChIP-seq data\n",
    "\n",
    "In this assignment you will build on the convolutional networks we looked at in class and work on ChIP-seq data for four transcription factors in arabidopsis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9313bb24-1ac1-4263-914f-cef4af557c3e",
   "metadata": {},
   "source": [
    "### Part 1: Data Preparation\n",
    "\n",
    "In this assignment you will work with ChIP-seq for three arabidopsis transcription factors:  AGL16, GRF1, and AMS.  The peaks that represent their binding sites in the arabidopsis genome are available in the following links:\n",
    "\n",
    "* AGL16 ([bed file](https://biobigdata.nju.edu.cn/ChIPHub_download/arabidopsis_thaliana/SRP187795/hammock/AGL16.target.all.bed.gz))\n",
    "* GRF1 ([bed file](https://biobigdata.nju.edu.cn/ChIPHub_download/arabidopsis_thaliana/SRP002566/hammock/SRX021610.peak.all.bed.gz))\n",
    "* AMS ([bed file](https://biobigdata.nju.edu.cn/ChIPHub_download/arabidopsis_thaliana/SRP188198/hammock/SRX5507861.peak.all.bed.gz))\n",
    "\n",
    "These files are in [bed format](https://en.wikipedia.org/wiki/BED_(file_format)), and contain the information on the genomic locations where the ChIP-seq peaks have been detected.  The linked wikipedia article provides the information you need about the format of these files.  Your task is to extract sequences of length 500 centered at the location of each peak, which you will provide as input to the convolutional network you train.  \n",
    "\n",
    "In order to extract the sequences associated with the peaks you will need the genomic sequence for arabidopsis.  This is available from the [Ensembl plants arabidopsis portal](https://plants.ensembl.org/Arabidopsis_thaliana/Info/Index).  In that page click on \"Download DNA sequence (FASTA)\", and the first five files provide the sequences for the five arabidopsis chromosomes.\n",
    "\n",
    "For reference, we computed the sequences associated with AGL16 peaks (link is in the assignment page in Canvas).\n",
    "\n",
    "Your final data preparation task is to prepare a labeled dataset with positive examples that correspond to the peak sequences.  As negative examples, use random permutations of the positive examples.  Create one permutation from each positive example.  How many examples did you obtain for each transcription factor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bebf5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task\n",
    "# - Load data\n",
    "# given bed file -> convert to fasta file (example database\\AGL16_peak_seq.fasta)\n",
    "# - Generate dataset\n",
    "# given a fasta file -> a dataset with negative example made from postive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c809572",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:7: SyntaxWarning: invalid escape sequence '\\A'\n",
      "<>:7: SyntaxWarning: invalid escape sequence '\\A'\n",
      "c:\\temp\\ipykernel_22200\\2856302313.py:7: SyntaxWarning: invalid escape sequence '\\A'\n",
      "  with open('database\\AGL16.target.all.bed') as f:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 4328 sequences and saved to FASTA file.\n"
     ]
    }
   ],
   "source": [
    "# lets look at the bed files \n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "\n",
    "positions = {}\n",
    "with open('database\\AGL16.target.all.bed') as f:\n",
    "    for line in f:\n",
    "        chr, start, stop, name, score, strand, thickStart, thickEnd, itemRgb, blockcount, blockSize, blockStart, _, _ = line.split()\n",
    "        # idk what the last two are there is only supposed to be 12 according to wikipedia\n",
    "        positions[name] = (chr, int(start), int(stop), (int(start)+int(stop))//2)\n",
    "\n",
    "genome_locations = [r'database\\Arabidopsis_thaliana.TAIR10.dna.chromosome.1.fa',\n",
    "                    r'database\\Arabidopsis_thaliana.TAIR10.dna.chromosome.2.fa',\n",
    "                    r'database\\Arabidopsis_thaliana.TAIR10.dna.chromosome.3.fa',\n",
    "                    r'database\\Arabidopsis_thaliana.TAIR10.dna.chromosome.4.fa',\n",
    "                    r'database\\Arabidopsis_thaliana.TAIR10.dna.chromosome.5.fa']\n",
    "genome = {}\n",
    "for l in genome_locations:\n",
    "    genome = genome | SeqIO.to_dict(SeqIO.parse(open(l), 'fasta'), lambda rec: 'Chr'+rec.name)\n",
    "\n",
    "# Extract 500bp sequences centered at midpoints\n",
    "sequences = []\n",
    "for name, (chr, start, stop, mid_point) in positions.items():\n",
    "    # Calculate the start and end positions for a 500bp sequence centered at midpoint\n",
    "    seq_start = max(0, mid_point - 250)  # Ensure we don't go below 0\n",
    "    seq_end = min(len(genome[chr].seq), mid_point + 250)  # Ensure we don't go beyond chromosome length\n",
    "    \n",
    "    # Extract the sequence\n",
    "    sequence = genome[chr].seq[seq_start:seq_end]\n",
    "    \n",
    "    # Create a SeqRecord\n",
    "    record = SeqRecord(sequence, id=name, description=f\"Chromosome {chr}, midpoint {mid_point}\")\n",
    "    sequences.append(record)\n",
    "\n",
    "# Write to FASTA file\n",
    "SeqIO.write(sequences, \"AGL16_peak_seq.fasta\", \"fasta\")\n",
    "print(f\"Extracted {len(sequences)} sequences and saved to FASTA file.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e4d11b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:8: SyntaxWarning: invalid escape sequence '\\A'\n",
      "<>:8: SyntaxWarning: invalid escape sequence '\\A'\n",
      "c:\\temp\\ipykernel_22200\\1604433860.py:8: SyntaxWarning: invalid escape sequence '\\A'\n",
      "  bed_path='database\\AGL16.target.all.bed',\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_sequences(genome_paths = [r'database\\Arabidopsis_thaliana.TAIR10.dna.chromosome.1.fa',\n",
    "                                                                r'database\\Arabidopsis_thaliana.TAIR10.dna.chromosome.2.fa',\n",
    "                                                                r'database\\Arabidopsis_thaliana.TAIR10.dna.chromosome.3.fa',\n",
    "                                                                r'database\\Arabidopsis_thaliana.TAIR10.dna.chromosome.4.fa',\n",
    "                                                                r'database\\Arabidopsis_thaliana.TAIR10.dna.chromosome.5.fa'], \n",
    "                                        bed_path='database\\AGL16.target.all.bed',\n",
    "                                        len_of_seqs=500):\n",
    "    \n",
    "    positions = {}\n",
    "    with open(bed_path) as f:\n",
    "        for line in f:\n",
    "            chr, start, stop, name, score, strand, thickStart, thickEnd, itemRgb, blockcount, blockSize, blockStart, _, _ = line.split()\n",
    "            # idk what the last two are there is only supposed to be 12 according to wikipedia\n",
    "            positions[name] = (chr, int(start), int(stop), (int(start)+int(stop))//2)\n",
    "\n",
    "    genome = {}\n",
    "    for l in genome_paths:\n",
    "        genome = genome | SeqIO.to_dict(SeqIO.parse(open(l), 'fasta'), lambda rec: 'Chr'+rec.name)\n",
    "\n",
    "    # Extract 500bp sequences centered at midpoints\n",
    "    sequences = []\n",
    "    for name, (chr, start, stop, mid_point) in positions.items():\n",
    "        # Calculate the start and end positions for a 500bp sequence centered at midpoint\n",
    "        seq_start = max(0, mid_point - len_of_seqs//2)  # Ensure we don't go below 0\n",
    "        seq_end = min(len(genome[chr].seq), mid_point + len_of_seqs//2)  # Ensure we don't go beyond chromosome length\n",
    "        \n",
    "        # Extract the sequence\n",
    "        sequence = genome[chr].seq[seq_start:seq_end]\n",
    "        \n",
    "        # Create a SeqRecord\n",
    "        record = SeqRecord(sequence, id=name, description=f\"Chromosome {chr}, midpoint {mid_point}\")\n",
    "        sequences.append(record)\n",
    "\n",
    "    # Write to FASTA file\n",
    "    SeqIO.write(sequences, os.path.basename(bed_path)+\".fasta\", \"fasta\")\n",
    "    print(f\"Extracted {len(sequences)} sequences and saved to FASTA file.\")\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c8c490a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "nucleotide_indexes = {'a':0, 't':1, 'g':2, 'c':3, 'w': (0,1), 'n': (0,1,2,3)}\n",
    "\n",
    "def one_hot_encode(seq):\n",
    "    oneHotEncode = np.zeros([4,len(seq)])\n",
    "    for i, n in enumerate(seq):\n",
    "        oneHotEncode[nucleotide_indexes[n.lower()], i] = 1\n",
    "    return oneHotEncode\n",
    "\n",
    "def generate_dataset(sequences):\n",
    "    positive_seqs = []\n",
    "    negative_seqs = []\n",
    "\n",
    "    for seq_record in sequences:\n",
    "        positive_seqs.append(one_hot_encode(str(seq_record.seq)))\n",
    "        \n",
    "        seq_list = list(str(seq_record.seq))\n",
    "        np.random.shuffle(seq_list)\n",
    "        permuted_seq = ''.join(seq_list)\n",
    "        negative_seqs.append(one_hot_encode(permuted_seq))\n",
    "\n",
    "    positive_labels = np.ones(len(positive_seqs))\n",
    "    negative_labels = np.zeros(len(negative_seqs))\n",
    "\n",
    "    X = positive_seqs + negative_seqs\n",
    "    y = np.concatenate([positive_labels, negative_labels])\n",
    "\n",
    "    indices = np.arange(len(X))\n",
    "    np.random.shuffle(indices)\n",
    "    X = [X[i] for i in indices]\n",
    "    y = y[indices]\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Dataset created with {len(positive_seqs)} positive examples and {len(negative_seqs)} negative examples\")\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa36a215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 4328 sequences and saved to FASTA file.\n",
      "Dataset created with 4328 positive examples and 4328 negative examples\n"
     ]
    }
   ],
   "source": [
    "X, y = generate_dataset(get_sequences())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bea7438",
   "metadata": {},
   "source": [
    "### Part 2:  \n",
    "\n",
    "As discussed in class, deeper networks with multiple layers of convolution *can* improve a network's performance.  Your task here is to extend the implementation provided in class to have three layers of convolution.  In addition, implement early stopping based on performance on the validation set (essentially, continue training until the validation loss stops decreasing).\n",
    "Finally, train each network two or three times, and choose the best performing network based on the performance on the validation set as the one to evaluate on the test set.\n",
    "In your experiments, set aside 20% of the data for testing, 20% for validation, and 60% for training.\n",
    "Compare the accuracy of your network to that of a one layer CNN.  Accuracy should be measured using the area under the ROC curve.  In the next part of the assignment you will get to tune its parameters to try and improve its performance.\n",
    "Note that for all datasets you should be able to obtain accuracy of around 0.9 or better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350bb653",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import sklearn as sk\n",
    "\n",
    "model = nn.Sequential(nn.Conv2d(in_channels=4, out_channels=3, kernel_size=(3, 1), stride=1),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Conv2d(in_channels=4, out_channels=3, kernel_size=(3, 1), stride=1),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Conv2d(in_channels=4, out_channels=3, kernel_size=(3, 1), stride=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5995190",
   "metadata": {},
   "source": [
    "### Part 3:  experiments with network architecture and hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba53c9e3",
   "metadata": {},
   "source": [
    "With the implementation you created in Part 2, your next task is to explore the space of hyperparameters and architecture choices to determine their effect on the performance of your three-layer network.  Choose three aspects of the network to explore (e.g. the learning rate, whether dropout is helpful, the choice of activation function, etc.).  Discuss your results.  Which aspects of the model seem to have the most effect on the accuracy of the network?  Do the best parameter values vary from dataset to dataset?  Is your three layer network able to match or exceed the performance of a single layer network?  Hint:  it should!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed095cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac7d3faa-2dd2-4bd5-bdae-b0480ae04839",
   "metadata": {},
   "source": [
    "### Coding and reporting your results\n",
    "\n",
    "In your notebook, I do not want to see repetitive code.  Such code belongs in a function!\n",
    "In your reporting, make sure your results are clearly presented.  I recommend using a table format, and your table can be populated by your code.  pandas DataFrame objects render nicely in Jupyter notebooks.  Here's an example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac72281-efc0-440d-bc0b-ecd991a7468d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = [\n",
    "    ['AGL16', 'Dropout', 0.2, 0.92 ],\n",
    "    ['GRF1', 'Dropout', 0.3, 0.9],\n",
    "]\n",
    "pd.DataFrame(data, columns = ['Dataset', 'Hyperparameter', 'Value', 'Accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8779163-cfbd-4f23-9cd8-e710dbdebd25",
   "metadata": {},
   "source": [
    "#### Grading\n",
    "\n",
    "```\n",
    "Part 1: dataset creation (20 pts)\n",
    "Part 2: implementation of three layer network, early stopping, and multiple training (40 pts)\n",
    "Part 3: experiments on network architecture (40 pts)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
